#!/usr/bin/env python3
"""
performance_benchmark.py

æ€§èƒ½å¯¹æ¯”è„šæœ¬ï¼šCPU vs GPU (RAPIDS) æ•°æ®å¤„ç†æ€§èƒ½æµ‹è¯•
æµ‹é‡ throughputã€latency å’Œèµ„æºä½¿ç”¨æƒ…å†µ
"""

import os
import sys
import time
import json
import argparse
import subprocess
import tempfile
from typing import Dict, List, Tuple
import pandas as pd

# å°è¯•å¯¼å…¥å¯è§†åŒ–åº“
try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    PLOT_AVAILABLE = True
except ImportError:
    PLOT_AVAILABLE = False
    print("âš ï¸ å¯è§†åŒ–åº“ä¸å¯ç”¨ï¼Œå°†è·³è¿‡å›¾è¡¨ç”Ÿæˆ")

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__)))

# å°è¯•å¯¼å…¥å¤„ç†å™¨
try:
    from cpu_process import FlightDataProcessor as CPUProcessor
    CPU_AVAILABLE = True
except ImportError:
    CPU_AVAILABLE = False
    print("âš ï¸ CPUå¤„ç†å™¨æœªæ‰¾åˆ°")

try:
    from rapids_process import RapidsFlightDataProcessor as GPUProcessor
    GPU_AVAILABLE = True
except ImportError:
    GPU_AVAILABLE = False
    print("âš ï¸ GPUå¤„ç†å™¨æœªæ‰¾åˆ°")

class PerformanceBenchmark:
    def __init__(self, dataset_sizes: List[str] = None):
        self.dataset_sizes = dataset_sizes or ['small', 'medium', 'large']
        self.results = {
            'cpu': {},
            'gpu': {}
        }
        
    def prepare_test_datasets(self, base_path: str) -> Dict[str, str]:
        """å‡†å¤‡ä¸åŒå¤§å°çš„æµ‹è¯•æ•°æ®é›†"""
        datasets = {}
        
        # æŸ¥æ‰¾æ‰€æœ‰CSVæ–‡ä»¶
        import glob
        pattern = os.path.join(base_path, '**', '*.csv')
        all_files = glob.glob(pattern, recursive=True)
        
        if not all_files:
            print(f"âŒ åœ¨ {base_path} ä¸‹æœªæ‰¾åˆ°CSVæ–‡ä»¶")
            return datasets
        
        total_files = len(all_files)
        
        # åˆ›å»ºä¸åŒå¤§å°çš„æ•°æ®é›†
        size_configs = {
            'small': min(10, max(1, total_files // 4)),
            'medium': min(30, max(5, total_files // 2)),
            'large': total_files
        }
        
        for size_name, file_count in size_configs.items():
            if file_count > 0:
                # åˆ›å»ºä¸´æ—¶ç›®å½•
                temp_dir = tempfile.mkdtemp(prefix=f'benchmark_{size_name}_')
                
                # å¤åˆ¶æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
                selected_files = all_files[:file_count]
                for i, src_file in enumerate(selected_files):
                    dst_file = os.path.join(temp_dir, f'file_{i:03d}.csv')
                    try:
                        import shutil
                        shutil.copy2(src_file, dst_file)
                    except Exception as e:
                        print(f"å¤åˆ¶æ–‡ä»¶å¤±è´¥: {e}")
                        continue
                
                datasets[size_name] = temp_dir
                print(f"âœ… åˆ›å»º {size_name} æ•°æ®é›†: {file_count} ä¸ªæ–‡ä»¶ -> {temp_dir}")
        
        return datasets
    
    def run_cpu_benchmark(self, input_path: str, output_file: str, 
                         aircraft_db: str = None, **kwargs) -> Dict:
        """è¿è¡ŒCPUåŸºå‡†æµ‹è¯•"""
        if not CPU_AVAILABLE:
            return {'error': 'CPUå¤„ç†å™¨ä¸å¯ç”¨'}
        
        print("ğŸ–¥ï¸  å¼€å§‹CPUåŸºå‡†æµ‹è¯•...")
        start_time = time.time()
        
        try:
            processor = CPUProcessor(
                min_rows=kwargs.get('min_rows', 1000),
                sample_size=kwargs.get('sample_size', 100)
            )
            
            metrics = processor.process_files(
                input_path=input_path,
                output_file=output_file,
                aircraft_db_path=aircraft_db
            )
            
            # ç¡®ä¿metricsä¸ä¸ºNone
            if metrics is None:
                metrics = {
                    'total_time': 0.0,
                    'throughput': 0.0,
                    'processed_aircraft': 0,
                    'extracted_features': 0
                }
            
            total_time = time.time() - start_time
            
            # å¢å¼ºæŒ‡æ ‡
            metrics.update({
                'method': 'CPU',
                'wall_time': total_time,
                'memory_efficient': True,
                'gpu_memory_used': 0
            })
            
            return metrics
            
        except Exception as e:
            print(f"CPUæµ‹è¯•å¼‚å¸¸: {e}")
            return {'error': f'CPUæµ‹è¯•å¤±è´¥: {e}'}
    
    def run_gpu_benchmark(self, input_path: str, output_file: str, 
                         aircraft_db: str = None, **kwargs) -> Dict:
        """è¿è¡ŒGPUåŸºå‡†æµ‹è¯•"""
        if not GPU_AVAILABLE:
            return {'error': 'GPUå¤„ç†å™¨ä¸å¯ç”¨'}
        
        print("ğŸš€ å¼€å§‹GPUåŸºå‡†æµ‹è¯•...")
        start_time = time.time()
        
        try:
            processor = GPUProcessor(
                min_rows=kwargs.get('min_rows', 1000),
                sample_size=kwargs.get('sample_size', 100),
                use_gpu=True
            )
            
            metrics = processor.process_files(
                input_path=input_path,
                output_file=output_file,
                aircraft_db_path=aircraft_db
            )
            
            # ç¡®ä¿metricsä¸ä¸ºNone
            if metrics is None:
                metrics = {
                    'total_time': 0.0,
                    'throughput': 0.0,
                    'processed_aircraft': 0,
                    'extracted_features': 0
                }
            
            total_time = time.time() - start_time
            
            # å¢å¼ºæŒ‡æ ‡
            metrics.update({
                'method': 'GPU',
                'wall_time': total_time,
                'memory_efficient': False,  # GPUé€šå¸¸ä½¿ç”¨æ›´å¤šå†…å­˜
                'gpu_memory_used': self._get_gpu_memory_usage()
            })
            
            return metrics
            
        except Exception as e:
            print(f"GPUæµ‹è¯•å¼‚å¸¸: {e}")
            return {'error': f'GPUæµ‹è¯•å¤±è´¥: {e}'}
    
    def _get_gpu_memory_usage(self) -> float:
        """è·å–GPUå†…å­˜ä½¿ç”¨é‡"""
        try:
            import cupy as cp
            mempool = cp.get_default_memory_pool()
            return mempool.used_bytes() / (1024**3)  # GB
        except:
            return 0.0
    
    def run_comparison(self, base_path: str, aircraft_db: str = None, 
                      output_dir: str = "benchmark_results") -> Dict:
        """è¿è¡Œå®Œæ•´çš„æ€§èƒ½å¯¹æ¯”"""
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„å¤„ç†å™¨
        if not CPU_AVAILABLE and not GPU_AVAILABLE:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„å¤„ç†å™¨")
            return {}
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®é›†
        print("ğŸ“ å‡†å¤‡æµ‹è¯•æ•°æ®é›†...")
        datasets = self.prepare_test_datasets(base_path)
        
        if not datasets:
            print("âŒ æ— æ³•åˆ›å»ºæµ‹è¯•æ•°æ®é›†")
            return {}
        
        # æµ‹è¯•å‚æ•°
        test_params = {
            'min_rows': 500,
            'sample_size': 100
        }
        
        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        for size_name, dataset_path in datasets.items():
            print(f"\n{'='*50}")
            print(f"ğŸ§ª æµ‹è¯•æ•°æ®é›†: {size_name.upper()}")
            print(f"{'='*50}")
            
            # CPUæµ‹è¯•
            if CPU_AVAILABLE:
                cpu_output = os.path.join(output_dir, f'cpu_{size_name}_features.csv')
                cpu_metrics = self.run_cpu_benchmark(
                    dataset_path, cpu_output, aircraft_db, **test_params
                )
                self.results['cpu'][size_name] = cpu_metrics
            else:
                self.results['cpu'][size_name] = {'error': 'CPUå¤„ç†å™¨ä¸å¯ç”¨'}
            
            # GPUæµ‹è¯•
            if GPU_AVAILABLE:
                gpu_output = os.path.join(output_dir, f'gpu_{size_name}_features.csv')
                gpu_metrics = self.run_gpu_benchmark(
                    dataset_path, gpu_output, aircraft_db, **test_params
                )
                self.results['gpu'][size_name] = gpu_metrics
            else:
                self.results['gpu'][size_name] = {'error': 'GPUå¤„ç†å™¨ä¸å¯ç”¨'}
            
            # å³æ—¶å¯¹æ¯”
            self._print_comparison(size_name, 
                                 self.results['cpu'][size_name], 
                                 self.results['gpu'][size_name])
        
        # ç”ŸæˆæŠ¥å‘Š
        self._generate_report(output_dir)
        if PLOT_AVAILABLE:
            self._generate_plots(output_dir)
        else:
            print("âš ï¸ è·³è¿‡å›¾è¡¨ç”Ÿæˆï¼ˆmatplotlib/seabornä¸å¯ç”¨ï¼‰")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        self._cleanup_temp_datasets(datasets)
        
        return self.results
    
    def _print_comparison(self, size: str, cpu_metrics: Dict, gpu_metrics: Dict):
        """æ‰“å°å•ä¸ªæµ‹è¯•çš„å¯¹æ¯”ç»“æœ"""
        print(f"\nğŸ“Š {size.upper()} æ•°æ®é›†ç»“æœ:")
        print("-" * 40)
        
        if 'error' in cpu_metrics:
            print(f"CPU: âŒ {cpu_metrics['error']}")
        else:
            print(f"CPU: âœ… {cpu_metrics.get('total_time', 0):.2f}s, "
                  f"{cpu_metrics.get('throughput', 0):.2f} ç‰¹å¾/ç§’")
        
        if 'error' in gpu_metrics:
            print(f"GPU: âŒ {gpu_metrics['error']}")
        else:
            print(f"GPU: âœ… {gpu_metrics.get('total_time', 0):.2f}s, "
                  f"{gpu_metrics.get('throughput', 0):.2f} ç‰¹å¾/ç§’")
        
        # è®¡ç®—åŠ é€Ÿæ¯”
        if ('error' not in cpu_metrics and 'error' not in gpu_metrics and 
            cpu_metrics.get('total_time', 0) > 0 and gpu_metrics.get('total_time', 0) > 0):
            speedup = cpu_metrics['total_time'] / gpu_metrics['total_time']
            print(f"âš¡ GPUåŠ é€Ÿæ¯”: {speedup:.2f}x")
    
    def _generate_report(self, output_dir: str):
        """ç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½æŠ¥å‘Š"""
        report_path = os.path.join(output_dir, 'performance_report.json')
        
        # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        summary = {
            'test_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'datasets_tested': list(self.results['cpu'].keys()),
            'cpu_available': CPU_AVAILABLE,
            'gpu_available': GPU_AVAILABLE,
            'detailed_results': self.results
        }
        
        # è®¡ç®—å¹³å‡åŠ é€Ÿæ¯”
        speedups = []
        for size in self.results['cpu'].keys():
            cpu_result = self.results['cpu'][size]
            gpu_result = self.results['gpu'][size]
            
            if ('error' not in cpu_result and 'error' not in gpu_result):
                cpu_time = cpu_result.get('total_time', 0)
                gpu_time = gpu_result.get('total_time', 0)
                if cpu_time > 0 and gpu_time > 0:
                    speedups.append(cpu_time / gpu_time)
        
        if speedups:
            summary['average_speedup'] = sum(speedups) / len(speedups)
            summary['max_speedup'] = max(speedups)
            summary['min_speedup'] = min(speedups)
        else:
            summary['average_speedup'] = None
        
        # ä¿å­˜æŠ¥å‘Š
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nğŸ“‹ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        # æ‰“å°æ±‡æ€»
        print(f"\n{'='*60}")
        print("ğŸ† æ€§èƒ½å¯¹æ¯”æ±‡æ€»")
        print(f"{'='*60}")
        if speedups:
            print(f"å¹³å‡GPUåŠ é€Ÿæ¯”: {summary['average_speedup']:.2f}x")
            print(f"æœ€å¤§GPUåŠ é€Ÿæ¯”: {summary['max_speedup']:.2f}x")
            print(f"æœ€å°GPUåŠ é€Ÿæ¯”: {summary['min_speedup']:.2f}x")
        else:
            print("æ— æ³•è®¡ç®—åŠ é€Ÿæ¯”ï¼ˆç¼ºå°‘æœ‰æ•ˆçš„å¯¹æ¯”æ•°æ®ï¼‰")
    
    def _generate_plots(self, output_dir: str):
        """ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å›¾è¡¨"""
        try:
            # å‡†å¤‡æ•°æ®
            plot_data = []
            for method in ['cpu', 'gpu']:
                for size, metrics in self.results[method].items():
                    if 'error' not in metrics:
                        plot_data.append({
                            'Method': method.upper(),
                            'Dataset Size': size,
                            'Total Time (s)': metrics.get('total_time', 0),
                            'Throughput (features/s)': metrics.get('throughput', 0),
                            'Features Extracted': metrics.get('extracted_features', 0)
                        })
            
            if not plot_data:
                print("âš ï¸ æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ç”Ÿæˆå›¾è¡¨")
                return
            
            df = pd.DataFrame(plot_data)
            
            # åˆ›å»ºå›¾è¡¨
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle('CPU vs GPU Performance Comparison', fontsize=16, fontweight='bold')
            
            # 1. å¤„ç†æ—¶é—´å¯¹æ¯”
            sns.barplot(data=df, x='Dataset Size', y='Total Time (s)', 
                       hue='Method', ax=axes[0,0])
            axes[0,0].set_title('Processing Time Comparison')
            axes[0,0].set_ylabel('Time (seconds)')
            
            # 2. ååé‡å¯¹æ¯”
            sns.barplot(data=df, x='Dataset Size', y='Throughput (features/s)', 
                       hue='Method', ax=axes[0,1])
            axes[0,1].set_title('Throughput Comparison')
            axes[0,1].set_ylabel('Features per Second')
            
            # 3. ç‰¹å¾æå–æ•°é‡
            sns.barplot(data=df, x='Dataset Size', y='Features Extracted', 
                       hue='Method', ax=axes[1,0])
            axes[1,0].set_title('Features Extracted')
            axes[1,0].set_ylabel('Number of Features')
            
            # 4. åŠ é€Ÿæ¯”
            speedup_data = []
            for size in df['Dataset Size'].unique():
                cpu_time = df[(df['Method'] == 'CPU') & (df['Dataset Size'] == size)]['Total Time (s)'].values
                gpu_time = df[(df['Method'] == 'GPU') & (df['Dataset Size'] == size)]['Total Time (s)'].values
                
                if len(cpu_time) > 0 and len(gpu_time) > 0 and gpu_time[0] > 0:
                    speedup = cpu_time[0] / gpu_time[0]
                    speedup_data.append({'Dataset Size': size, 'Speedup': speedup})
            
            if speedup_data:
                speedup_df = pd.DataFrame(speedup_data)
                sns.barplot(data=speedup_df, x='Dataset Size', y='Speedup', ax=axes[1,1])
                axes[1,1].set_title('GPU Speedup (CPU Time / GPU Time)')
                axes[1,1].set_ylabel('Speedup Factor')
                axes[1,1].axhline(y=1, color='red', linestyle='--', alpha=0.7, label='No Speedup')
                axes[1,1].legend()
            else:
                axes[1,1].text(0.5, 0.5, 'No speedup data available', 
                              ha='center', va='center', transform=axes[1,1].transAxes)
                axes[1,1].set_title('GPU Speedup')
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            plot_path = os.path.join(output_dir, 'performance_comparison.png')
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š æ€§èƒ½å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜: {plot_path}")
            
            # ä¿å­˜æ•°æ®è¡¨
            csv_path = os.path.join(output_dir, 'performance_data.csv')
            df.to_csv(csv_path, index=False)
            print(f"ğŸ“ˆ æ€§èƒ½æ•°æ®å·²ä¿å­˜: {csv_path}")
            
        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆå›¾è¡¨æ—¶å‡ºé”™: {e}")
    
    def _cleanup_temp_datasets(self, datasets: Dict[str, str]):
        """æ¸…ç†ä¸´æ—¶æ•°æ®é›†"""
        import shutil
        for size_name, temp_dir in datasets.items():
            try:
                shutil.rmtree(temp_dir)
                print(f"ğŸ§¹ å·²æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_dir}")
            except Exception as e:
                print(f"âš ï¸ æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥ {temp_dir}: {e}")

def main():
    parser = argparse.ArgumentParser(
        description="CPU vs GPU æ€§èƒ½å¯¹æ¯”åŸºå‡†æµ‹è¯•"
    )
    parser.add_argument(
        "input_path",
        help="è¾“å…¥æ•°æ®è·¯å¾„"
    )
    parser.add_argument(
        "--aircraft-db",
        help="èˆªç©ºå™¨æ•°æ®åº“è·¯å¾„",
        default=None
    )
    parser.add_argument(
        "--output-dir",
        help="è¾“å‡ºç›®å½•",
        default="benchmark_results"
    )
    parser.add_argument(
        "--dataset-sizes",
        nargs='+',
        help="è¦æµ‹è¯•çš„æ•°æ®é›†å¤§å°",
        default=['small', 'medium', 'large']
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥å¤„ç†å™¨å¯ç”¨æ€§
    if not CPU_AVAILABLE and not GPU_AVAILABLE:
        print("âŒ CPUå’ŒGPUå¤„ç†å™¨éƒ½ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥å®‰è£…")
        return
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    benchmark = PerformanceBenchmark(args.dataset_sizes)
    results = benchmark.run_comparison(
        base_path=args.input_path,
        aircraft_db=args.aircraft_db,
        output_dir=args.output_dir
    )
    
    print(f"\nğŸ‰ åŸºå‡†æµ‹è¯•å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {args.output_dir}")

if __name__ == "__main__":
    main()