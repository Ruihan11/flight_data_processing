#!/usr/bin/env python3
"""
cpu_parallel.py

å¹¶è¡ŒåŠ é€Ÿçš„èˆªç©ºæ•°æ®å¤„ç†è„šæœ¬ï¼Œä½¿ç”¨ Python çš„ multiprocessing
æ¨¡å—åœ¨ CPU ä¸ŠåŒæ—¶å¤„ç†å¤šä¸ªèˆªç©ºå™¨ã€‚æ•´ä½“æµç¨‹ä¸ cpu_process.py
ä¸€è‡´ï¼Œä½†åœ¨ç‰¹å¾æå–é˜¶æ®µå¯¹æ¯ä¸ªèˆªç©ºå™¨ ID åˆ†é…ç‹¬ç«‹çš„è¿›ç¨‹ä»¥
æå‡ååé‡ã€‚
"""

import os
import glob
import argparse
import pandas as pd
import numpy as np
import time
from typing import Any, Dict, List, Tuple, Optional
from multiprocessing import Pool, cpu_count

# å¯¼å…¥ç°æœ‰çš„ CPU å¤„ç†å™¨ä»¥å¤ç”¨é¢„å¤„ç†å’Œç‰¹å¾æå–é€»è¾‘
try:
    from cpu_process import FlightDataProcessor
except ImportError:
    # å¦‚æœæ¨¡å—ä¸å­˜åœ¨ï¼ŒæŠ›å‡ºå¼‚å¸¸
    raise ImportError("cpu_process.py æœªæ‰¾åˆ°ï¼Œæ— æ³•å¯¼å…¥ FlightDataProcessor")


def process_single_aircraft(args: Tuple[Any, List[pd.DataFrame], int, int]) -> List[Dict[str, Any]]:
    """åœ¨ä¸€ä¸ªç‹¬ç«‹è¿›ç¨‹ä¸­å¤„ç†å•ä¸ªèˆªç©ºå™¨çš„æ•°æ®ï¼Œè¿”å›ç‰¹å¾åˆ—è¡¨ã€‚"""
    aircraft_id, frames, min_rows, sample_size = args
    # ä¸ºæ¯ä¸ªè¿›ç¨‹åˆ›å»ºç‹¬ç«‹çš„å¤„ç†å™¨ï¼Œé¿å…å…±äº«çŠ¶æ€
    processor = FlightDataProcessor(min_rows=min_rows, sample_size=sample_size)
    try:
        # åˆå¹¶åŒä¸€èˆªç©ºå™¨çš„æ‰€æœ‰æ•°æ®
        combined_df = pd.concat(frames, ignore_index=True)
        combined_df = combined_df.drop_duplicates(subset=['Time'])
        # é¢„å¤„ç†
        processed_df = processor.preprocess_dataframe(combined_df)
        if processed_df is None:
            return []
        # ç‰¹å¾æå–
        features = processor.extract_features_from_flight(processed_df)
        return features
    except Exception as e:
        # å‘ç”Ÿå¼‚å¸¸æ—¶è¿”å›ç©ºåˆ—è¡¨ï¼ŒåŒæ—¶æ‰“å°è°ƒè¯•ä¿¡æ¯
        print(f"âš ï¸ å¹¶è¡Œå¤„ç†èˆªç©ºå™¨ {aircraft_id} æ—¶å‡ºé”™: {e}")
        return []


class ParallelFlightDataProcessor:
    """å¹¶è¡ŒåŒ–çš„ CPU æ•°æ®å¤„ç†å™¨ã€‚"""
    def __init__(self, min_rows: int = 1000, sample_size: int = 100, workers: Optional[int] = None):
        self.min_rows = min_rows
        self.sample_size = sample_size
        self.workers = workers or max(1, cpu_count() - 1)
        self.aircraft_db = None

    def load_aircraft_database(self, db_path: str) -> None:
        """åŠ è½½èˆªç©ºå™¨æ•°æ®åº“"""
        if os.path.exists(db_path):
            self.aircraft_db = pd.read_csv(db_path)
            self.aircraft_db['registration'] = (
                self.aircraft_db['registration']
                .astype(str).str.upper().str.strip()
            )
            print(f"âœ… èˆªç©ºå™¨æ•°æ®åº“å·²åŠ è½½: {len(self.aircraft_db)} æ¡è®°å½•")
        else:
            print(f"âš ï¸ èˆªç©ºå™¨æ•°æ®åº“æœªæ‰¾åˆ°: {db_path}")

    def add_aircraft_info(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ·»åŠ èˆªç©ºå™¨ä¿¡æ¯"""
        if self.aircraft_db is None:
            df['icao24'] = 'UNKNOWN'
            df['icaoaircrafttype'] = 'UNKNOWN'
            df['type_code'] = 1
            return df
        df['Tail'] = df['Tail'].astype(str).str.upper().str.strip()
        merged = df.merge(
            self.aircraft_db,
            left_on='Tail',
            right_on='registration',
            how='left'
        )
        merged['icao24'] = merged['icao24'].fillna('UNKNOWN')
        merged['icaoaircrafttype'] = merged['icaoaircrafttype'].fillna('UNKNOWN')
        merged['type_code'] = pd.factorize(merged['icaoaircrafttype'])[0] + 1
        if 'registration' in merged.columns:
            merged.drop(columns=['registration'], inplace=True)
        return merged

    def filter_outliers(self, df: pd.DataFrame) -> pd.DataFrame:
        """è¿‡æ»¤åŸºäºæŒç»­æ—¶é—´çš„å¼‚å¸¸å€¼"""
        duration_median = (
            df.groupby('ID')['duration']
            .median()
            .reset_index(name='median_duration')
        )
        df = df.merge(duration_median, on='ID')
        filtered = df[df['duration'] <= 1.5 * df['median_duration']].copy()
        filtered.drop(columns=['median_duration'], inplace=True)
        return filtered

    def process_files(self, input_path: str, output_file: str, aircraft_db_path: Optional[str] = None) -> Dict[str, float]:
        """å¤„ç†æ‰€æœ‰æ–‡ä»¶ï¼Œè¿”å›æ€§èƒ½æŒ‡æ ‡"""
        start_time = time.time()
        # åŠ è½½æ•°æ®åº“
        if aircraft_db_path:
            self.load_aircraft_database(aircraft_db_path)
        # æŸ¥æ‰¾ CSV æ–‡ä»¶
        pattern = os.path.join(input_path, '**', '*.csv')
        csv_files = glob.glob(pattern, recursive=True)
        if not csv_files:
            print(f"âŒ åœ¨ {input_path} ä¸‹æœªæ‰¾åˆ°CSVæ–‡ä»¶")
            return {
                'total_time': 0.0,
                'read_time': 0.0,
                'process_time': 0.0,
                'post_time': 0.0,
                'processed_aircraft': 0,
                'extracted_features': 0,
                'throughput': 0.0,
                'files_processed': 0
            }
        print(f"ğŸ“ æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")

        read_start = time.time()
        data_by_id: Dict[Any, List[pd.DataFrame]] = {}
        file_count = 0
        # é¡ºåºè¯»å–æ–‡ä»¶å¹¶åˆ†ç»„
        for csv_file in csv_files:
            file_count += 1
            try:
                df = pd.read_csv(csv_file, low_memory=False)
                if 'ID' not in df.columns:
                    continue
                for aircraft_id, group in df.groupby('ID'):
                    if aircraft_id not in data_by_id:
                        data_by_id[aircraft_id] = []
                    data_by_id[aircraft_id].append(group)
                if file_count % 20 == 0:
                    print(f"ğŸ“„ å·²è¯»å– {file_count}/{len(csv_files)} ä¸ªæ–‡ä»¶...")
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡æ–‡ä»¶ {os.path.basename(csv_file)}: {e}")
                continue
        read_time = time.time() - read_start
        print(f"âœ… æ–‡ä»¶è¯»å–å®Œæˆï¼Œå…±æ‰¾åˆ° {len(data_by_id)} ä¸ªä¸åŒçš„èˆªç©ºå™¨ID")

        # å¹¶è¡Œå¤„ç†æ¯ä¸ªèˆªç©ºå™¨
        process_start = time.time()
        all_features: List[Dict[str, Any]] = []
        processed_count = 0
        # æ„é€ ä»»åŠ¡åˆ—è¡¨
        tasks = [
            (aircraft_id, frames, self.min_rows, self.sample_size)
            for aircraft_id, frames in data_by_id.items()
        ]
        # ä½¿ç”¨ multiprocessing.Pool æ‰§è¡Œä»»åŠ¡
        with Pool(processes=self.workers) as pool:
            for features in pool.imap_unordered(process_single_aircraft, tasks):
                if features:
                    all_features.extend(features)
                    processed_count += 1
                    if processed_count % 10 == 0:
                        print(f"âœ… å·²å¹¶è¡Œå¤„ç† {processed_count} ä¸ªèˆªç©ºå™¨")
        process_time = time.time() - process_start

        if not all_features:
            print("âŒ æ²¡æœ‰æå–åˆ°ä»»ä½•ç‰¹å¾")
            return {
                'total_time': time.time() - start_time,
                'read_time': read_time,
                'process_time': process_time,
                'post_time': 0.0,
                'processed_aircraft': processed_count,
                'extracted_features': 0,
                'throughput': 0.0,
                'files_processed': len([f for f in csv_files if os.path.exists(f)])
            }

        # åå¤„ç†é˜¶æ®µ
        post_start = time.time()
        features_df = pd.DataFrame(all_features)
        # åˆå¹¶èˆªç©ºå™¨ä¿¡æ¯
        features_df = self.add_aircraft_info(features_df)
        # è¿‡æ»¤å¼‚å¸¸å€¼
        features_df = self.filter_outliers(features_df)
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        # ä¿å­˜ç»“æœ
        features_df.to_csv(output_file, index=False)
        post_time = time.time() - post_start

        total_time = time.time() - start_time
        print(f"ğŸ‰ å¹¶è¡Œå¤„ç†å®Œæˆ!")
        print(f"   - å¤„ç†äº† {processed_count} ä¸ªèˆªç©ºå™¨")
        print(f"   - æå–äº† {len(features_df)} ä¸ªç‰¹å¾å—")
        print(f"   - ç»“æœä¿å­˜è‡³: {output_file}")

        return {
            'total_time': total_time,
            'read_time': read_time,
            'process_time': process_time,
            'post_time': post_time,
            'processed_aircraft': processed_count,
            'extracted_features': len(features_df),
            'throughput': len(features_df) / total_time if total_time > 0 else 0,
            'files_processed': len([f for f in csv_files if os.path.exists(f)])
        }


def main() -> None:
    parser = argparse.ArgumentParser(
        description="å¹¶è¡ŒåŠ é€Ÿçš„èˆªç©ºæ•°æ®å¤„ç†è„šæœ¬"
    )
    parser.add_argument(
        "input_path",
        help="è¾“å…¥æ•°æ®è·¯å¾„ï¼ˆæ”¯æŒé€’å½’æœç´¢CSVæ–‡ä»¶ï¼‰"
    )
    parser.add_argument(
        "output_file",
        help="è¾“å‡ºç‰¹å¾æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--aircraft-db",
        help="èˆªç©ºå™¨æ•°æ®åº“CSVæ–‡ä»¶è·¯å¾„",
        default=None
    )
    parser.add_argument(
        "--min-rows",
        type=int,
        default=1000,
        help="æœ€å°è¡Œæ•°é˜ˆå€¼ (é»˜è®¤: 1000)"
    )
    parser.add_argument(
        "--sample-size",
        type=int,
        default=100,
        help="æ¯ä¸ªç‰¹å¾å—çš„æ ·æœ¬æ•° (é»˜è®¤: 100)"
    )
    parser.add_argument(
        "--workers",
        type=int,
        default=None,
        help="å¹¶è¡Œè¿›ç¨‹æ•°é‡ (é»˜è®¤: CPU æ ¸å¿ƒæ•° - 1)"
    )
    args = parser.parse_args()

    processor = ParallelFlightDataProcessor(
        min_rows=args.min_rows,
        sample_size=args.sample_size,
        workers=args.workers,
    )
    processor.process_files(
        input_path=args.input_path,
        output_file=args.output_file,
        aircraft_db_path=args.aircraft_db
    )


if __name__ == "__main__":
    main()